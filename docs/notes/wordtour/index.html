<!DOCTYPE html>
<html lang="ja" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Word Tour | HideOut</title>
<meta name="keywords" content="論文読み, NLP">
<meta name="description" content="単語埋め込みを健全性を保持したまま一次元へ次元削減する手法の提案．World ではない． 資料 Sato, R.. (2022). Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem. (NAACL 2022) NLP コロキウム資料 最適">
<meta name="author" content="">
<link rel="canonical" href="https://ivorypawn.github.io/notes/wordtour/">
<link crossorigin="anonymous" href="https://ivorypawn.github.io/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ivorypawn.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ivorypawn.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ivorypawn.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ivorypawn.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ivorypawn.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>




<style>
    @media screen and (min-width: 769px) {
         
        .post-content input[type="checkbox"]:checked ~ label > img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style>


<link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
<meta property="og:title" content="Word Tour" />
<meta property="og:description" content="単語埋め込みを健全性を保持したまま一次元へ次元削減する手法の提案．World ではない． 資料 Sato, R.. (2022). Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem. (NAACL 2022) NLP コロキウム資料 最適" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ivorypawn.github.io/notes/wordtour/" /><meta property="article:section" content="notes" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word Tour"/>
<meta name="twitter:description" content="単語埋め込みを健全性を保持したまま一次元へ次元削減する手法の提案．World ではない． 資料 Sato, R.. (2022). Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem. (NAACL 2022) NLP コロキウム資料 最適"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://ivorypawn.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Word Tour",
      "item": "https://ivorypawn.github.io/notes/wordtour/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Word Tour",
  "name": "Word Tour",
  "description": "単語埋め込みを健全性を保持したまま一次元へ次元削減する手法の提案．World ではない． 資料 Sato, R.. (2022). Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem. (NAACL 2022) NLP コロキウム資料 最適",
  "keywords": [
    "論文読み", "NLP"
  ],
  "articleBody": "単語埋め込みを健全性を保持したまま一次元へ次元削減する手法の提案．World ではない．\n資料 Sato, R.. (2022). Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem. (NAACL 2022) NLP コロキウム資料 最適輸送の使いかた 導入 自然言語の計算機可読表現をどのように実装するか考える．\n“사과 (sagwa)” という語の意味を知りたいとする．“사과” のコーパスを引くと，たとえば以下の用例が見つかる．\n사과 の⽊を植える 冷え冷えの 사과 ジュースがうまい ＊＊県は 사과 の⽣産⾼が⽇本⼀ $\\{$ 木，ジュース，＊＊県，… $\\}$ は “사과” の 共起表現 と呼ばれる．また 分布仮説 とは『ある単語の意味はその共起表現を⾒ればわかる』というものである．\n分布仮説に基づいて，共起する単語を予測できるような計算機可読表現を作ることを考えたい．\n単語の集合 $\\mathcal{V}$ に対し，写像 $\\mathcal{V}\\rightarrow\\mathbb{R}^d;\\:w\\mapsto v$ を 単語埋め込み といい，$v$ を 単語ベクトル という．以下とくに混乱がなければ，単語ベクトルと単語を同一視する．\n（数学用語のベクトルや埋め込みを想起させるが，空間の線形性や距離の保存などは陽には考えないことが多い）\nこのとき単語ベクトル $v_w$ に対してほかの単語が共起する確率 $p(\\cdot|w)$ を定式化し，コーパスで共起する単語は近くに埋め込まれるように設計されるのが望ましい．\n代表的な単語埋め込みの word2vec は，3 層 NN を ~100 billion words コーパスで学習している．共起確率は以下で定式化される：\n$$ p(c|w ; \\theta)=\\frac{e^{v_{c} \\cdot v_{w}}}{\\sum_{c^{\\prime} \\in C} e^{v_{c^{\\prime}} \\cdot v_{w}}} $$\n⼈間が感じる意味の類似度と word2vec による単語ベクトルペアのなす $\\cos$ 類似度は⾼い相関を示した．word2vec はたとえば gensim から利用できる．\nBERT はコーパスに現れる文章の一部をマスクして穴埋め問題を解かせまくる学習で，語順も考慮した周辺文脈の獲得を目指した．これが成功し NLP の広範なアプリケーションで飛躍的に性能が向上した．\n自然言語処理におけるEmbeddingの方法一覧とサンプルコード 課題 分布仮説に基づく⾃然⾔語処理でうまく対処できない⾔語現象は存在するが，それとは別に実用上の課題がある．\n現実に使用される単語埋め込みは典型的には $d=300$ 程度であり，メモリ・実行時間の効率面に課題がある．現状 $400,000 \\text{ words}\\times 300\\fallingdotseq1\\text{GB}$ をスマホに載せるのはつらい．\nまた高次元に埋め込まれた単語の解釈可能性という問題がある．t-SNE や PCA で次元削減して可視化したところで，各成分に何らかの解釈ができる望みはほぼない．これは画像処理における GAN で得られる特徴量が，ある方向への射影が何らかの解釈を齎すこととは対照的である（e.g. 肌の色，表情）[Unsupervised Discovery of Interpretable Directions in the GAN Latent Space. ICML 2020. etc.]．\n単語ベクトルに摂動を加えて adversarial example や data augmentation を生成する手法も提案されているが，たとえば “$\\text{cat}$” の単語ベクトルに摂動を加えた $v_\\text{cat}+\\epsilon$ は，単語レベルでは意味を解釈できない．これは解釈可能なデータが画像と違って単語の場合は離散的であることによる．\n内容 以下，word tour の論文の内容について．\n健全性，完全性 論文では，一般に単語埋め込みが満たしてほしい性質を 2 つ挙げている．\n健全性：単語ベクトルの距離が近ければ，単語の意味は近い 完全性：単語の意味が近ければ，単語ベクトルの距離は近い ポアンカレ埋め込みの教訓より，どちらかを諦めれば大きく精度劣化せず解釈可能性の高い次元削減が得られる可能性がある．論文の手法は健全性のみを採用する．すなわち取りこぼしはあり得るが，距離の近い 2 点の意味はつねに近いことは保証されており，単語検索・文書検索などに応用できる可能性がある．\nこれらの用語は恐らく NLP において一般的ではなく，論文筆者の造語で，論理学の概念に由来している．例えば古典命題論理の形式体系 $\\text{LK}$ の健全性／完全性は以下のように定義される．\n健全性：$\\text{LK}$ で証明可能な式はトートロジーである 完全性：トートロジーは $\\text{LK}$ で証明可能である $\\text{LK}$ において，これらはどちらも証明される．ただし上記の完全性は，厳密には意味論的完全性と呼ばれる概念である．有名なゲーデルの（第一）不完全性定理のステートメントにおける完全性は構文論的完全性であり，（関連はあるが）これとは別物である．\n「証明可能」は「単語ベクトルの距離が近い」，「トートロジー」は「単語の意味が近い」に対応する．\n巡回セールスマン問題 1 次元への次元削減を以下のように定式化する．\nInput 単語の集合 $\\mathcal{V}$ $n:=|\\mathcal{V}|$ 学習ずみ単語埋め込み $\\{x_v\\mid v\\in\\mathcal{V}\\}\\subset\\mathbb{R}^d$ 実験では $d=300$ 次元 Glove Output 健全な一次元埋め込み $\\sigma:\\mathcal{V}\\rightarrow{1,2,\\ldots,n}$ （全単射） minimize $\\sum_{i=1}^{n}\\left|x_{\\sigma^{-1}(i)}-x_{\\sigma^{-1}(i+1)}\\right|$ ただし $\\sigma^{-1}(n+1)=\\sigma^{-1}(1)$ とする（端の単語を特別視しない） 座標や距離は考えず，並び順のみを考える 超軽量（40,000 単語で 312 KB） 解釈可能 この問題設定は巡回セールスマン問題にほかならない．NP 困難で，厳密解法は素朴な動的計画法より優れたものは知られておらず， $O(2^n n^2)$ かかる（$n\\leq 20$ くらいまでなら解ける）．\n指数時間アルゴリズム入門 いっぽう近似解法については，100,000 点の入力に対して誤差 0.0019% の近似解が現実的な時間で得られることもある．\nKernighan–Lin algorithm Simplified LKH(Lin-Kernighan Heuristic)の実装 論文では，40,000 単語からなる埋め込み空間の点群を上記 Lin-Kernighan Heuristic ソルバーに投げ，理論下界 236300.947 に対し総距離 236882.314 となる解を得たとのこと．\n評価 https://github.com/joisino/wordtour#-results WordTour から適当な区間を切り出したものを眺めると，他手法を圧倒して解釈性が高い．とくに序数詞を教師なしで取り出せている点は面白く，Glove がそのような構造を含んでいることがわかる．\n文書（単語の集合）の類似度評価では，WordTour 上で「ぼかして」BoW (Bag of Words) を適用する手法を提案している（カーネル密度推定に似ている）．通常の BoW と比べて類似度を十分考慮できるため，精度が向上する．\nFYI: Word Mover’s Distance 比較対象である WMD (Word Mover’s Distance) は，埋め込み空間の点群間の最適輸送問題を解いている．\nもとの WMD の定式化は NLP ドメイン知識を踏まえるとやや不自然なところがあり，これを修正したところスコアが跳ね上がったという報告がある．\nWord Rotator’s Distance. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.2944–2960, 2020. 点群の各点に単語ベクトルのノルムに比例する重みを付与 輸送コストの距離関数を L2 から cos 類似度に もとの単語埋め込みの学習は原点依存 Kaggle 位置情報のプラットフォーマーである Foursquare のデータを題材に，店舗や施設の名前・住所・座標・カテゴリ・URL・電話番号が含まれるレコードが与えられ，POI が同一のものをマッチングするという kaggle コンペが開催された．\n同一 POI であっても，座標が少し違ったり，名前や住所の表記が異なっていたり，欠損があったりと，情報に揺らぎがある．\nこのコンペで WordTour を活用し，solution を公開した人が現れた．\n7th place solution(with inference code) Embedding を対照学習で生成したのち k-means++ で 2000 のクラスタに分割し，centroid の集合に WordTour を適用することで，後段の GBDT の学習が少ない弱学習器（決定木）で効率的に進んだとしている．\n残念ながらコンペ終了後に leakage 発覚し，上位モデルは過学習の疑いがもたれた．\nLeakage (67% of test rows leaked - more than anyone expected.) その後 Food-101 に対して WordTour を検証したノートブックが公開された．\nword tour experiment LKH ソルバーではなく google の OR-Tools を使っており，遅いが手軽に計算できるとのこと．WordTour によって学習に必要な iteration が半減している．\nWordTour と GBDT の親和性が高いのは直観的に納得できるし，面白いアイデアなので，leak-free コンペでよい結果が出てほしい．\n",
  "wordCount" : "2967",
  "inLanguage": "ja",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ivorypawn.github.io/notes/wordtour/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "HideOut",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ivorypawn.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ivorypawn.github.io/" accesskey="h" title="HideOut (Alt + H)">HideOut</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ivorypawn.github.io/posts/" title="posts">
                    <span>posts</span>
                </a>
            </li>
            <li>
                <a href="https://ivorypawn.github.io/notes/" title="notes">
                    <span>notes</span>
                </a>
            </li>
            <li>
                <a href="https://ivorypawn.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Word Tour
    </h1>
    <div class="post-meta">

</div>
  </header>

  
<div class="toc side left">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#資料">資料</a></li>
    <li><a href="#導入">導入</a>
      <ul>
        <li><a href="#課題">課題</a></li>
      </ul>
    </li>
    <li><a href="#内容">内容</a>
      <ul>
        <li><a href="#健全性完全性">健全性，完全性</a></li>
        <li><a href="#巡回セールスマン問題">巡回セールスマン問題</a>
          <ul>
            <li><a href="#input">Input</a></li>
            <li><a href="#output">Output</a></li>
          </ul>
        </li>
        <li><a href="#評価">評価</a>
          <ul>
            <li><a href="#fyi-word-movers-distance">FYI: Word Mover&rsquo;s Distance</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#kaggle">Kaggle</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>




 
  <div class="post-content"><p>単語埋め込みを健全性を保持したまま一次元へ次元削減する手法の提案．World ではない．</p>
<h2 id="資料">資料<a hidden class="anchor" aria-hidden="true" href="#資料">#</a></h2>
<ol>
<li>Sato, R.. (2022). <a href="https://arxiv.org/abs/2205.01954">Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem</a>. (NAACL 2022)</li>
<li><a href="https://www.slideshare.net/joisino/word-tour-onedimensional-word-embeddings-via-the-traveling-salesman-problem-naacl-2022">NLP コロキウム資料</a></li>
<li><a href="https://speakerdeck.com/eumesy/how-to-leverage-optimal-transport">最適輸送の使いかた</a></li>
</ol>
<h2 id="導入">導入<a hidden class="anchor" aria-hidden="true" href="#導入">#</a></h2>
<p>自然言語の計算機可読表現をどのように実装するか考える．</p>
<p>&ldquo;사과 (sagwa)&rdquo; という語の意味を知りたいとする．&ldquo;사과&rdquo; のコーパスを引くと，たとえば以下の用例が見つかる．</p>
<ul>
<li>사과 の⽊を植える</li>
<li>冷え冷えの 사과 ジュースがうまい</li>
<li>＊＊県は 사과 の⽣産⾼が⽇本⼀</li>
</ul>
<p>$\{$ 木，ジュース，＊＊県，&hellip; $\}$ は &ldquo;사과&rdquo; の <strong>共起表現</strong> と呼ばれる．また <strong>分布仮説</strong> とは『ある単語の意味はその共起表現を⾒ればわかる』というものである．</p>
<p>分布仮説に基づいて，共起する単語を予測できるような計算機可読表現を作ることを考えたい．</p>
<p>単語の集合 $\mathcal{V}$ に対し，写像 $\mathcal{V}\rightarrow\mathbb{R}^d;\:w\mapsto v$ を <strong>単語埋め込み</strong> といい，$v$ を <strong>単語ベクトル</strong> という．以下とくに混乱がなければ，単語ベクトルと単語を同一視する．</p>
<p>（数学用語のベクトルや埋め込みを想起させるが，空間の線形性や距離の保存などは陽には考えないことが多い）</p>
<p>このとき単語ベクトル $v_w$ に対してほかの単語が共起する確率 $p(\cdot|w)$ を定式化し，コーパスで共起する単語は近くに埋め込まれるように設計されるのが望ましい．</p>
<p>代表的な単語埋め込みの <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> は，3 層 NN を ~100 billion words コーパスで学習している．共起確率は以下で定式化される：</p>
<p>$$
p(c|w ; \theta)=\frac{e^{v_{c} \cdot v_{w}}}{\sum_{c^{\prime} \in C} e^{v_{c^{\prime}} \cdot v_{w}}}
$$</p>
<p>⼈間が感じる意味の類似度と word2vec による単語ベクトルペアのなす $\cos$ 類似度は⾼い相関を示した．word2vec はたとえば <a href="https://radimrehurek.com/gensim/index.html">gensim</a> から利用できる．</p>
<p><a href="https://ledge.ai/bert/">BERT</a> はコーパスに現れる文章の一部をマスクして穴埋め問題を解かせまくる学習で，語順も考慮した周辺文脈の獲得を目指した．これが成功し NLP の広範なアプリケーションで飛躍的に性能が向上した．</p>
<ul>
<li><a href="https://yukoishizaki.hatenablog.com/entry/2020/01/03/175156">自然言語処理におけるEmbeddingの方法一覧とサンプルコード</a></li>
</ul>
<h3 id="課題">課題<a hidden class="anchor" aria-hidden="true" href="#課題">#</a></h3>
<p>分布仮説に基づく⾃然⾔語処理でうまく対処できない⾔語現象は存在するが，それとは別に実用上の課題がある．</p>
<p>現実に使用される単語埋め込みは典型的には $d=300$ 程度であり，メモリ・実行時間の効率面に課題がある．現状 $400,000 \text{ words}\times 300\fallingdotseq1\text{GB}$ をスマホに載せるのはつらい．</p>
<p>また高次元に埋め込まれた単語の解釈可能性という問題がある．t-SNE や PCA で次元削減して可視化したところで，各成分に何らかの解釈ができる望みはほぼない．これは画像処理における GAN で得られる特徴量が，ある方向への射影が何らかの解釈を齎すこととは対照的である（e.g. 肌の色，表情）[Unsupervised Discovery of Interpretable Directions in the GAN Latent Space. ICML 2020. etc.]．</p>
<p>単語ベクトルに摂動を加えて adversarial example や data augmentation を生成する手法も提案されているが，たとえば &ldquo;$\text{cat}$&rdquo; の単語ベクトルに摂動を加えた $v_\text{cat}+\epsilon$ は，単語レベルでは意味を解釈できない．これは解釈可能なデータが画像と違って単語の場合は離散的であることによる．</p>
<h2 id="内容">内容<a hidden class="anchor" aria-hidden="true" href="#内容">#</a></h2>
<p>以下，word tour の論文の内容について．</p>
<h3 id="健全性完全性">健全性，完全性<a hidden class="anchor" aria-hidden="true" href="#健全性完全性">#</a></h3>
<p>論文では，一般に単語埋め込みが満たしてほしい性質を 2 つ挙げている．</p>
<ul>
<li><strong>健全性</strong>：単語ベクトルの距離が近ければ，単語の意味は近い</li>
<li><strong>完全性</strong>：単語の意味が近ければ，単語ベクトルの距離は近い</li>
</ul>
<p>ポアンカレ埋め込みの教訓より，どちらかを諦めれば大きく精度劣化せず解釈可能性の高い次元削減が得られる可能性がある．論文の手法は健全性のみを採用する．すなわち取りこぼしはあり得るが，距離の近い 2 点の意味はつねに近いことは保証されており，単語検索・文書検索などに応用できる可能性がある．</p>
<p>これらの用語は恐らく NLP において一般的ではなく，論文筆者の造語で，論理学の概念に由来している．例えば古典命題論理の形式体系 $\text{LK}$ の健全性／完全性は以下のように定義される．</p>
<ul>
<li>健全性：$\text{LK}$ で証明可能な式はトートロジーである</li>
<li>完全性：トートロジーは $\text{LK}$ で証明可能である</li>
</ul>
<p>$\text{LK}$ において，これらはどちらも証明される．ただし上記の完全性は，厳密には意味論的完全性と呼ばれる概念である．有名なゲーデルの（第一）不完全性定理のステートメントにおける完全性は構文論的完全性であり，（関連はあるが）これとは別物である．</p>
<p>「証明可能」は「単語ベクトルの距離が近い」，「トートロジー」は「単語の意味が近い」に対応する．</p>
<h3 id="巡回セールスマン問題">巡回セールスマン問題<a hidden class="anchor" aria-hidden="true" href="#巡回セールスマン問題">#</a></h3>
<p>1 次元への次元削減を以下のように定式化する．</p>
<h4 id="input">Input<a hidden class="anchor" aria-hidden="true" href="#input">#</a></h4>
<ul>
<li>単語の集合 $\mathcal{V}$
<ul>
<li>$n:=|\mathcal{V}|$</li>
</ul>
</li>
<li>学習ずみ単語埋め込み $\{x_v\mid v\in\mathcal{V}\}\subset\mathbb{R}^d$
<ul>
<li>実験では $d=300$ 次元 Glove</li>
</ul>
</li>
</ul>
<h4 id="output">Output<a hidden class="anchor" aria-hidden="true" href="#output">#</a></h4>
<ul>
<li>健全な一次元埋め込み $\sigma:\mathcal{V}\rightarrow{1,2,\ldots,n}$ （全単射）</li>
<li>minimize $\sum_{i=1}^{n}\left|x_{\sigma^{-1}(i)}-x_{\sigma^{-1}(i+1)}\right|$
<ul>
<li>ただし $\sigma^{-1}(n+1)=\sigma^{-1}(1)$ とする（端の単語を特別視しない）</li>
</ul>
</li>
<li>座標や距離は考えず，並び順のみを考える
<ul>
<li>超軽量（40,000 単語で 312 KB）</li>
<li>解釈可能</li>
</ul>
</li>
</ul>
<p>この問題設定は巡回セールスマン問題にほかならない．NP 困難で，厳密解法は素朴な動的計画法より優れたものは知られておらず， $O(2^n n^2)$ かかる（$n\leq 20$ くらいまでなら解ける）．</p>
<ul>
<li><a href="https://www.slideshare.net/wata_orz/ss-12131479">指数時間アルゴリズム入門</a></li>
</ul>
<p>いっぽう近似解法については，100,000 点の入力に対して誤差 0.0019% の近似解が現実的な時間で得られることもある．</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Kernighan%E2%80%93Lin_algorithm">Kernighan–Lin algorithm</a></li>
<li><a href="http://shopping2.gmobb.jp/htdmnr/www08/np/tsp/tsp_slkh01.html">Simplified LKH(Lin-Kernighan Heuristic)の実装</a></li>
</ul>
<p>論文では，40,000 単語からなる埋め込み空間の点群を上記 Lin-Kernighan Heuristic ソルバーに投げ，理論下界 236300.947 に対し総距離 236882.314 となる解を得たとのこと．</p>
<h3 id="評価">評価<a hidden class="anchor" aria-hidden="true" href="#評価">#</a></h3>
<ul>
<li><a href="https://github.com/joisino/wordtour#-results">https://github.com/joisino/wordtour#-results</a></li>
</ul>
<p>WordTour から適当な区間を切り出したものを眺めると，他手法を圧倒して解釈性が高い．とくに序数詞を教師なしで取り出せている点は面白く，Glove がそのような構造を含んでいることがわかる．</p>
<p>文書（単語の集合）の類似度評価では，WordTour 上で「ぼかして」BoW (Bag of Words) を適用する手法を提案している（カーネル密度推定に似ている）．通常の BoW と比べて類似度を十分考慮できるため，精度が向上する．</p>
<h4 id="fyi-word-movers-distance">FYI: Word Mover&rsquo;s Distance<a hidden class="anchor" aria-hidden="true" href="#fyi-word-movers-distance">#</a></h4>
<p>比較対象である WMD (Word Mover&rsquo;s Distance) は，埋め込み空間の点群間の最適輸送問題を解いている．</p>
<p>もとの WMD の定式化は NLP ドメイン知識を踏まえるとやや不自然なところがあり，これを修正したところスコアが跳ね上がったという報告がある．</p>
<ul>
<li><a href="https://arxiv.org/abs/2004.15003">Word Rotator&rsquo;s Distance</a>. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.2944–2960, 2020.
<ul>
<li>点群の各点に単語ベクトルのノルムに比例する重みを付与</li>
<li>輸送コストの距離関数を L2 から cos 類似度に
<ul>
<li>もとの単語埋め込みの学習は原点依存</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="kaggle">Kaggle<a hidden class="anchor" aria-hidden="true" href="#kaggle">#</a></h2>
<p>位置情報のプラットフォーマーである Foursquare のデータを題材に，店舗や施設の名前・住所・座標・カテゴリ・URL・電話番号が含まれるレコードが与えられ，POI が同一のものをマッチングするという kaggle コンペが開催された．</p>
<p>同一 POI であっても，座標が少し違ったり，名前や住所の表記が異なっていたり，欠損があったりと，情報に揺らぎがある．</p>
<p>このコンペで WordTour を活用し，solution を公開した人が現れた．</p>
<ul>
<li><a href="https://www.kaggle.com/competitions/foursquare-location-matching/discussion/335800">7th place solution(with inference code)</a></li>
</ul>
<p>Embedding を対照学習で生成したのち k-means++ で 2000 のクラスタに分割し，centroid の集合に WordTour を適用することで，後段の GBDT の学習が少ない弱学習器（決定木）で効率的に進んだとしている．</p>
<p>残念ながらコンペ終了後に leakage 発覚し，上位モデルは過学習の疑いがもたれた．</p>
<ul>
<li><a href="https://www.kaggle.com/competitions/foursquare-location-matching/discussion/335799">Leakage (67% of test rows leaked - more than anyone expected.)</a></li>
</ul>
<p>その後 Food-101 に対して WordTour を検証したノートブックが公開された．</p>
<ul>
<li><a href="https://www.kaggle.com/code/nadare/word-tour-experiment/notebook">word tour experiment</a></li>
</ul>
<p>LKH ソルバーではなく google の OR-Tools を使っており，遅いが手軽に計算できるとのこと．WordTour によって学習に必要な iteration が半減している．</p>
<p>WordTour と GBDT の親和性が高いのは直観的に納得できるし，面白いアイデアなので，leak-free コンペでよい結果が出てほしい．</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ivorypawn.github.io/tags/%E8%AB%96%E6%96%87%E8%AA%AD%E3%81%BF/">論文読み</a></li>
      <li><a href="https://ivorypawn.github.io/tags/nlp/">NLP</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://ivorypawn.github.io/notes/design/">
    <span class="title">« Prev</span>
    <br>
    <span>デザインに関する覚書</span>
  </a>
  <a class="next" href="https://ivorypawn.github.io/notes/wavelet-matrix/">
    <span class="title">Next »</span>
    <br>
    <span>Wavelet Matrix</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://ivorypawn.github.io/">HideOut</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
